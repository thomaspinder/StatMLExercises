{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZFoHxypfzIso"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "import optax as ox\n",
    "from jax.scipy.linalg import cho_factor, cho_solve\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    ")\n",
    "cols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "key = jr.PRNGKey(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdtg_t8SztlH"
   },
   "source": [
    "## Kernels\n",
    "\n",
    "For a variance $\\sigma^2$ and lengthscale $\\ell$, we can write the squared exponential kernel as\n",
    "\n",
    "$$\n",
    "k(x, x') =\n",
    "\\sigma^2 \\exp\\left(\\frac{-0.5\\lVert x - x' \\rVert_2^2}{\\ell^2}\\right) .\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "The corresponding JAX implementation of this is given by\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJ8V5EACzcVV",
    "outputId": "b7aa833b-5a53-42ad-aee3-20da0755d1e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.27303721, dtype=float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rbf_kernel(x, y, params):\n",
    "    ell, sigma = params[\"lengthscale\"], params[\"variance\"]\n",
    "    tau = jnp.sum(jnp.square(x / ell - y / ell))\n",
    "    return sigma * jnp.exp(-0.5 * tau)\n",
    "\n",
    "\n",
    "x1 = jnp.array(2.0)\n",
    "x2 = jnp.array(0.9)\n",
    "params = {\"lengthscale\": jnp.array(1.0), \"variance\": jnp.array(0.5)}\n",
    "rbf_kernel(x1, x2, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXM5cp3n0R1r"
   },
   "source": [
    "Similarly, the Matern 1/2 kernel is given by\n",
    "\n",
    "$$\n",
    "k(x, x') = \\sigma^2\\exp\\Bigg(-\\frac{\\lvert x-x' \\rvert}{\\ell}\\Bigg). \\tag{2}\n",
    "$$\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "Implement the Mat√©rn 1/2 kernel from (2) in JAX.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "nmkzoJev0zkj"
   },
   "outputs": [],
   "source": [
    "# Answer Here\n",
    "def matern12_kernel(x, y, params):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8Opq6VG5GVj"
   },
   "source": [
    "Check:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "DMvXhorp1iK4"
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(matern12_kernel(x1, x2, params), jnp.array(0.16643554))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qone2I-5ISj"
   },
   "source": [
    "## Computing the covariance\n",
    "\n",
    "For a set of $N$ data points, the covariance matrix is an $N\\times N$ matrix $\\mathbf{K}$ that is given by evaluating the kernel function pairwise on all inputs i.e.,\n",
    "\n",
    "$$\n",
    "\\left[\\mathbf{K} \\right]_{ij} = k(x_i, x_j)\\,, \\quad \\text{for} \\ i,j \\in 1,2,\\ldots, N.\n",
    "$$\n",
    "\n",
    "In regular Python, we may be tempted to implement this using for-loops i.e.,\n",
    "\n",
    "```\n",
    "K = np.zeros(N, N)\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        K[i, j] = kernel_fn(x[i], x[j])\n",
    "```\n",
    "\n",
    "However, such operations are generally avoided in JAX and we should instead use `vmap` ([documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html)).\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "Write a function to compute the covariance matrix using the below function signature. Hint, you will need two vmaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "fXSNOFKV5rMv"
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "X = jnp.sort(jr.uniform(key, shape=(N, 1), minval=-3.0, maxval=3), axis=0)\n",
    "\n",
    "\n",
    "def evaluate_kernel(kernel_fn, x, y, params):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZnMrfg27cDn"
   },
   "source": [
    "## Sampling from the GP\n",
    "\n",
    "With a covariance function now computed, we may sample from the GP prior. The below figure shows 20 samples from a GP prior where the GP is parameterised by and RBF kernel with lengthscale 1., and variance 0.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "tOvEoN7o8FFa"
   },
   "outputs": [],
   "source": [
    "def stabilise_covariance(K, jitter):\n",
    "    N = K.shape[0]\n",
    "    eye = jnp.eye(N)\n",
    "    jitter_matrix = eye * jitter\n",
    "    return K + jitter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "RIwInDbc0JRO",
    "outputId": "b9fca50b-e7c7-4ec7-b99d-bad5ec7be65f"
   },
   "outputs": [],
   "source": [
    "k = evaluate_kernel(rbf_kernel, X, X, params)\n",
    "mu = jnp.zeros(N)\n",
    "dist = tfp.distributions.MultivariateNormalTriL(\n",
    "    mu, jnp.linalg.cholesky(stabilise_covariance(k))\n",
    ")\n",
    "samples = dist.sample(seed=key, sample_shape=(10)).T\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(samples)\n",
    "ax.set(xlabel=\"$x$\", ylabel=r\"$p(f(x))$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mni0FPOX8peI"
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "Recreate the above figure using your Matern kernel. What do you notice about the samples?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JncEbfl_9JZr"
   },
   "source": [
    "## Conditioning a GP\n",
    "\n",
    "Now that we've defined a GP prior, let's condition on some data and compute the posterior distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "H-Om-Avx82cn"
   },
   "outputs": [],
   "source": [
    "def predict(x, y, params):\n",
    "    n = x.shape[0]\n",
    "    Kxx = evaluate_kernel(rbf_kernel, x, x, params[\"kernel\"])\n",
    "    Kxx += jnp.eye(n) * params[\"likelihood\"][\"obs_noise\"]\n",
    "    prior_mean = jnp.zeros_like(x)\n",
    "    L = cho_factor(Kxx, lower=True)\n",
    "    prior_distance = y - prior_mean\n",
    "    weights = cho_solve(L, prior_distance)\n",
    "\n",
    "    def mean_and_variance(test_points):\n",
    "        Kfx = evaluate_kernel(rbf_kernel, x, test_points, params[\"kernel\"])\n",
    "        mu = jnp.dot(Kfx.T, weights)\n",
    "        Kxx = evaluate_kernel(rbf_kernel, test_points, test_points, params[\"kernel\"])\n",
    "        latents = cho_solve(L, Kfx)\n",
    "        return mu, Kxx - jnp.dot(Kfx.T, latents)\n",
    "\n",
    "    return mean_and_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-03BjLX9mPv"
   },
   "source": [
    "Let us now extend our parameters and compute the conditional mean and (co)variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "W7bDLIZn9nJB"
   },
   "outputs": [],
   "source": [
    "true_obs_noise = 0.25\n",
    "\n",
    "params = {\n",
    "    \"kernel\": {\"lengthscale\": jnp.array(0.25), \"variance\": jnp.array(0.5)},\n",
    "    \"likelihood\": {\"obs_noise\": jnp.array(1.0)},\n",
    "}\n",
    "\n",
    "\n",
    "true_f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\n",
    "y = true_f(X)\n",
    "y += jr.normal(key, shape=y.shape) * true_obs_noise\n",
    "\n",
    "x_test = jnp.linspace(-3.2, 3.2, num=300)[:, None]\n",
    "y_test = true_f(x_test)\n",
    "mu, sigma2 = predict(X, y, params)(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "JnxGFpA1_OeV",
    "outputId": "f98589e5-e290-4410-d28a-e2dc06a2a33e"
   },
   "outputs": [],
   "source": [
    "def plot_conditional(x_train, y_train, x_test, mu, sigma2, ax=None, legend=False):\n",
    "    cols = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(figsize=(7, 3))\n",
    "    ax.plot(x_train, y_train, \"x\", label=\"Observations\", color=cols[0])\n",
    "    ax.plot(x_test, mu, label=\"Predictive mean\", linewidth=2, color=cols[1])\n",
    "    ax.fill_between(\n",
    "        x_test.squeeze(),\n",
    "        mu.squeeze() - 3 * jnp.sqrt(jnp.diag(sigma2).squeeze()),\n",
    "        mu.squeeze() + 3 * jnp.sqrt(jnp.diag(sigma2).squeeze()),\n",
    "        alpha=0.1,\n",
    "        color=cols[1],\n",
    "        label=r\"3 $\\sigma$\",\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        x_test.squeeze(),\n",
    "        mu.squeeze() - jnp.sqrt(jnp.diag(sigma2).squeeze()),\n",
    "        mu.squeeze() + jnp.sqrt(jnp.diag(sigma2).squeeze()),\n",
    "        alpha=0.3,\n",
    "        color=cols[1],\n",
    "        label=r\"1 $\\sigma$\",\n",
    "    )\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set(xlabel=r\"$x$\", ylabel=r\"$f(x)$\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "ax = plot_conditional(X, y, x_test, mu, sigma2)\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hyWfPMn9R-U"
   },
   "source": [
    "## Exercise 4\n",
    "\n",
    "Experiment with different values for the parameters defined above. What effect does this have on the conditional mean and variance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gk3X5wYGDug2"
   },
   "source": [
    "## Optimising the Hyperparameter\n",
    "\n",
    "From your solution to Exercise 4, you can see that the fidelity between the observed data and the conditional distribution varies with the parameters' values. However, identifying the _optimal_ value through grid search is not a scalable solution and we seek to instead obtain Type-II MLEs. We do this through gradient-based optimisation.\n",
    "\n",
    "Each of the three parameters considered here are strictly positive. Because of this, there is a danger when optimising them that we could step too far and arrive at a negative value. For this reason, we transform our parameters so that they are defined on the entire real line, make an optimisation step, and then back-transform the parameter value so that they are re-defined on their original constrained space. We therefore require that our function is bijective i.e., has a one-to-one mapping, so that the transforming and back-transforming steps are fully identifiable. This is easily achieved with JAX's `tree_map` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKNFQVmiEI03",
    "outputId": "4195a8aa-bda1-41b5-c302-b89a9ceee7a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': {'lengthscale': Array(0.54132485, dtype=float64, weak_type=True), 'variance': Array(0.54132485, dtype=float64, weak_type=True)}, 'likelihood': {'obs_noise': Array(0.54132485, dtype=float64, weak_type=True)}}\n"
     ]
    }
   ],
   "source": [
    "softplus = lambda x: jnp.log(1.0 + jnp.exp(x))\n",
    "inv_softplus = lambda x: jnp.log(jnp.exp(x) - 1.0)\n",
    "\n",
    "unconstrained_params = jax.tree_map(inv_softplus, params)\n",
    "print(unconstrained_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78Y69gFdEuAc"
   },
   "source": [
    "With parameters now defined on the entire real line, we‚Äôll now go ahead and define our GP‚Äôs marginal log-likelihood function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbq46LozEdFL"
   },
   "outputs": [],
   "source": [
    "def marginal_log_likelihood(x, y):\n",
    "    n = x.shape[0]\n",
    "    mu = jnp.zeros(shape=x.shape[0])\n",
    "\n",
    "    def objective(params, jitter_amount):\n",
    "        params = jax.tree_map(softplus, params)\n",
    "        Kff = evaluate_kernel(rbf_kernel, x, x, params[\"kernel\"])\n",
    "        noise_matrix = jnp.eye(n) * params[\"likelihood\"][\"obs_noise\"]\n",
    "        gram_matrix = Kff + noise_matrix + jnp.eye(n) * jitter_amount\n",
    "        L = jnp.linalg.cholesky(gram_matrix)\n",
    "        return (\n",
    "            jnp.array(-1.0)\n",
    "            * tfp.distributions.MultivariateNormalTriL(mu, L)\n",
    "            .log_prob(y.squeeze())\n",
    "            .sum()\n",
    "        )\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDWFIEzYEyZc"
   },
   "source": [
    "In the above function, one can see that we use TensorFlow Probability to evaluate the log-probability density function of a multivariate normal distribution. This is by no means a requirement, and one can certainly define their own log-pdf, however, this is a nice point to show how TensorFlow Porbability can be seamlessly integrated into Jax code.\n",
    "\n",
    "## Exercise 5\n",
    "\n",
    "Compute the gradient of the marginal log-likelihood function and evaluate it for your current unconstrained set of parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHLpcJ0LFEq-"
   },
   "source": [
    "## Exercise 6\n",
    "\n",
    "We'll want to evaluate the MLL multiple times during our optimisation. In such cases, it is advantageous to JIT compile the function. JIT compile your MLL function using JAX's `jit` function and compare the runtimes with and without jitting using the `%timeit` magic command. Note - you will need to use the `block_until_ready()` function to get accurate timings e.g.,\n",
    "\n",
    "```{py}\n",
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVITCQAWFogy"
   },
   "source": [
    "Happy that we may differentiate and compile our MLL, let's now perform optimisation against it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEwrxSabEdrH"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"kernel\": {\"lengthscale\": jnp.array(1), \"variance\": jnp.array(1)},\n",
    "    \"likelihood\": {\"obs_noise\": jnp.array(1.0)},\n",
    "}\n",
    "params = jax.tree_map(inv_softplus, params)\n",
    "\n",
    "optimizer = ox.adam(learning_rate=0.05)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "mll = marginal_log_likelihood(X, y)\n",
    "\n",
    "for _ in range(500):\n",
    "    grads = jax.grad(mll)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = ox.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jmx5QAFHYfN",
    "outputId": "e049e3d9-ff99-4774-ea69-3d5c093b3fd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': {'lengthscale': Array(0.46292848, dtype=float64),\n",
       "  'variance': Array(1.46747712, dtype=float64)},\n",
       " 'likelihood': {'obs_noise': Array(0.04353514, dtype=float64)}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_params = jax.tree_map(softplus, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "gMuwbDYYHlWn",
    "outputId": "5bcd935d-f7b7-41d9-c705-069b6aa8670c"
   },
   "outputs": [],
   "source": [
    "mu, sigma = predict(X, y, mle_params)(x_test)\n",
    "\n",
    "ax = plot_conditional(X, y, x_test, mu, sigma)\n",
    "ax.plot(x_test, true_f(x_test), label=\"True function\", color=cols[3], linestyle=\"--\")\n",
    "ax.legend(loc=\"best\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
